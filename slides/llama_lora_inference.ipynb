{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a2cbca2-8873-42e9-b50a-a6b802552143",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LLM mit LoRA verwenden\n",
    "## Am Beispiel deutscher Zitate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac11ec7-adbc-4998-991d-9c4ae820e561",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vorbereiten des Grundmodells\n",
    "\n",
    "Im folgenden wird das Grundmodell geladen. Dieses muss zum LORA Adapter passen..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfe0ce4-7ebc-446a-8154-d69dcb53011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8853b3c7-670c-4d1b-b252-bb01838cdeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"TheBloke/leo-hessianai-7B-GPTQ\"\n",
    "#base_model_name = \"LeoLM/leo-hessianai-7b\"\n",
    "# Variante des Modells\n",
    "revision = \"gptq-8bit-32g-actorder_True\" \n",
    "#revision = \"main\"\n",
    "\n",
    "refined_model = \"german_quotes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544161f2-cba2-4379-a900-6025e2072c3d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Quantization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    revision=revision,\n",
    "    #quantization_config=quant_config,\n",
    "    #device_map={\"\": 0}  # uncomment for GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e60579-c627-4947-ba30-2dd4c1a69c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, \n",
    "                                          trust_remote_code=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Fix for fp16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a66c0-673a-40d3-a8d2-e33661987768",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vorbereiten der Textpipeline\n",
    "\n",
    "Wichtig ist wieder der Prompt: Dieser sollte möglichst mit dem Prompt im Training übereinstimmen. \n",
    "Hier wird der User-Input auf den Autor des zu generierenden Zitats reduziert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae125dc4-a2ef-4e74-9318-a7ccbca56d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_gen = pipeline(task=\"text-generation\", model=base_model,\n",
    "                    max_length=200, tokenizer=tokenizer)\n",
    "\n",
    "system_prompt = \"\"\"Dies ist eine Unterhaltung zwischen \\\n",
    "einem intelligenten, hilfsbereitem \\\n",
    "KI-Assistenten und einem Nutzer.\n",
    "Der Assistent gibt Antworten in Form von Zitaten.\"\"\"\n",
    "prompt_format = \"<|im_start|>system\\n{system_prompt}\\\n",
    "<|im_end|>\\n<|im_start|>user\\nZitiere {prompt}\\\n",
    "<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "def get_quote(author:str, max_length:int=130):\n",
    "    query = prompt_format.format(system_prompt=system_prompt, prompt= author)\n",
    "    output = text_gen(query, do_sample=True, top_p=0.95, max_length=max_length, \n",
    "                      return_full_text=False, pad_token_id=tokenizer.pad_token_id)\n",
    "    print(output[0]['generated_text'])\n",
    "\n",
    "base_model.load_adapter(refined_model, adapter_name=refined_model)\n",
    "# Lädt das Lora Modell in den Speicher\n",
    "base_model.enable_adapters() # Aktiviert das LORA Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d863000",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Anfragen absetzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609964b2-a43e-4b91-98a0-4259f1f4c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_quote(\"Christoph\")\n",
    "get_quote(\"Oscar Wilde\")\n",
    "get_quote(\"Aristoteles\")\n",
    "get_quote(\"Heinrich Heine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a273d2-840b-4514-ac0a-92b158ada157",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.disable_adapters() # Deaktiviert das LORA Modell\n",
    "get_quote(\"Christoph\")\n",
    "get_quote(\"Oscar Wilde\")\n",
    "get_quote(\"Aristoteles\")\n",
    "get_quote(\"Heinrich Heine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1fdd31-f780-440b-b57a-1ed4e15e55dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Alternative\n",
    "Ist das Modell wie beschrieben bei HuggingFace hochgeladen, dann geht das Laden deutlich schneller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35a885ec-9cc3-4d7a-8574-305fc7eb8b49",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from ctransformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from transformers import pipeline\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"caretech-owl/leo-hessionai-7B-quotes-gguf\", hf=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
