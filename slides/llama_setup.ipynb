{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8983a092-8fd7-45a8-a591-e954b0232393",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Llama 2 - Setup\n",
    "\n",
    "Llama ist ein offenes Sprachmodell, welches von Meta entwickelt und trainiert wurde.\n",
    "Auf Basis des Vorgängers, der wohl unabsichtlicht im Internet aufgetaucht ist, wurden nicht nur von Wissenschaffenden leistungsfähige Derivate entwickelt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Konfiguration\n",
    "\n",
    "Der GitHub-Nutzer `oobabooga` hat durch seine webbasierte Nutzungsschnittstelle die Einrichtung und den Betrieb von LLama-Modellen stark vereinfacht. In dem hier vorgestellten Arbeitsablauf, muss nur der Installationsort festgelegt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe9749c-bbcf-41fc-8f73-9e40bb56f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Where should the webui be downloaded to?\n",
    "LLAMA_ROOT = Path(\"../webui\")\n",
    "\n",
    "# This is only relevant for remote hosting\n",
    "# GRADIO_USER = __user__\n",
    "# GRADIO_PASS = __password__\n",
    "# !rm -rf {LLAMA_ROOT} && mkdir -p {LLAMA_ROOT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plattformspezifische Anpassungen\n",
    "\n",
    "Im besten Fall werden die restlichen Parameter automatisch gesetzt. Sollte das jedoch nicht funktionieren, können Nutzende die Umgebungsvariablen für den folgenden Installer auch händisch setzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import subprocess\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    OS = \"macos\"\n",
    "elif platform.system() == \"Windows\":\n",
    "    OS = \"windows\"\n",
    "else:\n",
    "    OS = \"linux\"\n",
    "\n",
    "INSTALLER = LLAMA_ROOT.joinpath(f\"start_{OS}.{'bat' if OS == 'windows' else 'sh'}\")\n",
    "\n",
    "try:\n",
    "    subprocess.check_output('nvidia-smi')\n",
    "    GPU_CHOICE = \"A\"  # use CUDA\n",
    "except Exception:\n",
    "    if platform.machine() == \"arm\":\n",
    "        GPU_CHOICE = \"C\"  # use MPS for M generation Macs\n",
    "    else:\n",
    "        GPU_CHOICE = \"N\"  # use CPU only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text Generation Web UI herunterladen\n",
    "\n",
    "Es ist wieder Zeit ein Projekt zu 'klonen'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83197c9-6377-4bf8-b9f2-fd25e02361fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T10:26:57.182183Z",
     "iopub.status.busy": "2023-11-23T10:26:57.181383Z",
     "iopub.status.idle": "2023-11-23T10:26:59.109526Z",
     "shell.execute_reply": "2023-11-23T10:26:59.108314Z",
     "shell.execute_reply.started": "2023-11-23T10:26:57.182151Z"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/oobabooga/text-generation-webui.git {LLAMA_ROOT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text Generation Web UI starten\n",
    "\n",
    "Wir nutzen den 'One-Click-Installer', den `oobabooga`.\n",
    "Dieser lädt `conda` zwar erneut herunter, was aber in Anbetracht der Größe der herunterzuladenden Modelle nicht weiter ins Gewicht fällt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6e25d2-40b0-4c17-978c-25716a166455",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T11:12:07.448490Z",
     "iopub.status.busy": "2023-11-23T11:12:07.447635Z"
    }
   },
   "outputs": [],
   "source": [
    "%env LLAMA_ROOT {LLAMA_ROOT}\n",
    "%env GPU_CHOICE {GPU_CHOICE}\n",
    "# If you know you have an AMD GPU and are on Linux or MacOS, you can set \n",
    "# GPU_CHOICE manually to \"B\" or \"D\" if you got an Intel Arc (IPEX) GPU\n",
    "# %env GPU_CHOICE = \"B\"\n",
    "%env USE_CUDA118 \"N\"  # if you have issues set this one to \"Y\"\n",
    "%env INSTALLER {INSTALLER}\n",
    "# This is only needed for remote hosting\n",
    "# %env COMMANDLINE_ARGS --share --gradio-auth {GRADIO_USER}:{GRADIO_PASS}\n",
    "\n",
    "!cd ${LLAMA_ROOT} && ${INSTALLER} ${COMMANDLINE_ARGS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wir brauchen Modelle\n",
    "\n",
    "Die Web UI erlaubt es, Sprachmodelle direkt aus der Weboberfläche von [Hugging Face](https://huggingface.co) (HF) herunterzuladen.\n",
    "Dazu muss im Reiter `Model` nur der sogenannte HF-Handle eingegeben werden.\n",
    "Sollte der Handle nicht eindeutig sein, kann noch die spezifische Modeldatei angegeben werden.\n",
    "\n",
    "Die Auswahl an Sprachmodellen ist unüberschauber und neu trainierte Modelle werden täglich veröffentlicht.\n",
    "Eine Orientierung bietet das [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) von Hugging Face, welches Sprachmodelle und deren Leistungsfähigkeit in Benchark-Tests gegenüberstellt\n",
    "\n",
    "|Handle|File|\n",
    "|------|----|\n",
    "|[Intel/neural-chat-7b-v3-1](https://huggingface.co/Intel/neural-chat-7b-v3-1)|---|\n",
    "|[LeoLM/leo-hessianai-13b](https://huggingface.co/LeoLM/leo-hessianai-13b)|---|\n",
    "|[TheBloke/leo-hessianai-7B-GGUF](https://huggingface.co/TheBloke/leo-hessianai-7B-GGUF)|leo-hessianai-7b.Q5_K_M.gguf|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
