{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8983a092-8fd7-45a8-a591-e954b0232393",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Llama 2 - Setup\n",
    "\n",
    "Llama ist ein offenes Sprachmodell, welches von Meta entwickelt und trainiert wurde.\n",
    "Auf Basis des Vorgängers, der wohl unabsichtlicht im Internet aufgetaucht ist, wurden nicht nur von Wissenschaffenden leistungsfähige Derivate entwickelt.\n",
    "Der GitHub-Nutzer [oobabooga](https://github.com/oobabooga) hat durch seine webbasierte Nutzungsschnittstelle namens [text-generation-webui](https://github.com/oobabooga/text-generation-webui) die Einrichtung und die Bereitstellung von LLama-Modellen stark vereinfacht. Ein weiteres populares Projekt [GPT4All](https://gpt4all.io/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Konfiguration\n",
    "\n",
    "In dem hier vorgestellten Arbeitsablauf, muss nur der Installationsort festgelegt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe9749c-bbcf-41fc-8f73-9e40bb56f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Where should the webui be downloaded to?\n",
    "LLAMA_ROOT = Path(\"../text_gen\").resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plattformspezifische Anpassungen\n",
    "\n",
    "Im besten Fall werden die restlichen Parameter automatisch gesetzt. Sollte das jedoch nicht funktionieren, können Nutzende die Umgebungsvariablen für den folgenden Installer auch händisch setzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import subprocess\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    OS = \"macos\"\n",
    "elif platform.system() == \"Windows\":\n",
    "    OS = \"windows\"\n",
    "else:\n",
    "    OS = \"linux\"\n",
    "\n",
    "INSTALLER = LLAMA_ROOT.joinpath(f\"start_{OS}.{'bat' if OS == 'windows' else 'sh'}\")\n",
    "\n",
    "try:\n",
    "    subprocess.check_output(\"nvidia-smi\")\n",
    "    GPU_CHOICE = \"A\"  # use CUDA\n",
    "except Exception:\n",
    "    if OS == \"macos\" and platform.machine() == \"arm64\":\n",
    "        GPU_CHOICE = \"C\"  # use MPS for M generation Macs\n",
    "    else:\n",
    "        GPU_CHOICE = \"N\"  # use CPU only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text Generation Web UI herunterladen\n",
    "\n",
    "Es ist wieder Zeit ein Projekt zu 'klonen'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83197c9-6377-4bf8-b9f2-fd25e02361fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T10:26:57.182183Z",
     "iopub.status.busy": "2023-11-23T10:26:57.181383Z",
     "iopub.status.idle": "2023-11-23T10:26:59.109526Z",
     "shell.execute_reply": "2023-11-23T10:26:59.108314Z",
     "shell.execute_reply.started": "2023-11-23T10:26:57.182151Z"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/oobabooga/text-generation-webui.git {LLAMA_ROOT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text Generation Web UI starten\n",
    "\n",
    "Wir nutzen den 'One-Click-Installer', den `oobabooga`.\n",
    "Dieser lädt `conda` zwar erneut herunter, was aber in Anbetracht der Größe der herunterzuladenden Modelle nicht weiter ins Gewicht fällt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6e25d2-40b0-4c17-978c-25716a166455",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T11:12:07.448490Z",
     "iopub.status.busy": "2023-11-23T11:12:07.447635Z"
    }
   },
   "outputs": [],
   "source": [
    "%env GPU_CHOICE {GPU_CHOICE}\n",
    "# If you know you have an AMD GPU and are on Linux or MacOS, you can set \n",
    "# GPU_CHOICE manually to \"B\" or \"D\" if you got an Intel Arc (IPEX) GPU\n",
    "# %env GPU_CHOICE = B\n",
    "%env USE_CUDA118 N  # if you have issues set this one to \"Y\"\n",
    "# This is only needed for remote hosting\n",
    "# %env COMMANDLINE_ARGS --share --gradio-auth {GRADIO_USER}:{GRADIO_PASS}\n",
    "\n",
    "!{INSTALLER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wir brauchen Modelle\n",
    "\n",
    "<div style=\"text-align: center; margin-top: 1rem;\">\n",
    "   <img src=\"images/huggingface.svg\" width=\"80\"><span style=\"margin-left: 1rem; font-size: 4rem; font-weight:400; vertical-align: bottom;\">Hugging Face</span>\n",
    "</div>\n",
    "\n",
    "> Hugging Face, the most popular AI tool in Data Science, has been an attention magnet for 316.6 million traffic between September 2022 and August 2023. -- writerbuddy.ai [¹](https://writerbuddy.ai/blog/ai-industry-analysis)\n",
    "\n",
    "Die wahrscheinlich aktuell größte Community mit der größten Sammlung an GAN-Modellen findet man auf [huggingface.co](https://huggingface.co). Die Seite wird vom US-amerikanischen Unternehmen [Hugging Face, Inc](https://en.wikipedia.org/wiki/Hugging_Face) betreut.\n",
    "\n",
    "Die Web UI erlaubt es, Sprachmodelle direkt von [Hugging Face](https://huggingface.co) (HF) herunterzuladen.\n",
    "Dazu muss im Reiter `Model` nur der sogenannte HF-Handle eingegeben werden.\n",
    "Sollte der Handle nicht eindeutig sein, kann noch die spezifische Modeldatei angegeben werden.\n",
    "\n",
    "Die Auswahl an Sprachmodellen ist unüberschauber und neu trainierte Modelle werden täglich veröffentlicht.\n",
    "Eine Orientierung bietet das [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) von Hugging Face, welches Sprachmodelle und deren Leistungsfähigkeit in Benchark-Tests gegenüberstellt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Eine Modellauswahl\n",
    "\n",
    "* [Intel/neural-chat-7b-v3-1](https://huggingface.co/Intel/neural-chat-7b-v3-1)\n",
    "  + aktuell *im Schnitt* eines der besten 'generischen' Modelle\n",
    "* [LeoLM/leo-hessianai-13b](https://huggingface.co/LeoLM/leo-hessianai-13b)\n",
    "  + speziell für die deutsche Sprache optimiert\n",
    "* [TheBloke/leo-hessianai-7B-GGUF](https://huggingface.co/TheBloke/leo-hessianai-7B-GGUF)\n",
    "  + File: leo-hessianai-7b.Q5_K_M.gguf\n",
    "  + Alternative: [TheBloke/leo-hessianai-7B-GPTQ:gptq-8bit-32g-actorder_True](https://huggingface.co/TheBloke/leo-hessianai-7B-GPTQ)\n",
    "  + quantisiertes Version von LeoLM\n",
    "* [TheBloke/openinstruct-mistral-7B-GPTQ:gptq-8bit-32g-actorder_True](https://huggingface.co/TheBloke/openinstruct-mistral-7B-GPTQ)\n",
    "  + aktuell *im Schnitt* eines der besten Modelle speziell für Anweisungen trainiert\n",
    "  + quantisierte Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Anmerkungen zu dem Namensschema von Modellen\n",
    "\n",
    "Viele Modelle gibt es nicht nur in einer Variante. Um es einfacher zu machen, um welche Variante es sich handelt, gibt es einige Namenszusätze: `13B`/`7B`/`30B` gibt die Anzahl der Parameter des Modells an. Mehr bedeutet grob gesprochen genauere Ergebnisse, aber auch komplexere Berechnungen und einen größeren Speicherbedarf.\n",
    "\n",
    "Neben der Reduktion der Parameter, ist ein weiterer Optimierungsschritt die Quantisierung.\n",
    "Hierbei werden Parameter und Gewichte in Zahlenformate überführt die weniger Speicher benötigen.\n",
    "Dadurch wird die Genauigkeit der Berechnungen reduzierert, was jedoch bis zu einem gewissen Grad die Ausgabe der Modelle nur marginal negativ beeinflusst: `GGML` und `GGUF` geben bspw. Quantisierungsmethode an, wobei `GGUF` nach und nach `GGML` ersetzt. Einige Bibliotheken haben die Unterstützung von `GGML` bereits eingestellt. Die Art der für Quantifizierungsmethode und die Anzahl der genutzen Bits bei `GGUF`/`GGML` wird teils als weitere Zusätze. Bspw. steht `Q5_K_M` für 5 genutzte Bits und `GGML_TYPE_Q6_K`-Quantizierung [¹](https://github.com/ggerganov/llama.cpp/pull/1684)). Weitere Quantisierungs- und Komprimierungsmethoden `AWQ` und `GPTQ`, die teils mit einem stärken Fokus auf GPU-Berechnungen weitere Effizienzgewinne bedeuten können.\n",
    "\n",
    "Andere Zusätze wie `chat` oder `instruct` implizieren, dass die Modelle speziell für Konversationsszenarien oder zur Ausgabe bzw. zum Befolgen von Instruktionen trainiert/feinjustiert worden sind."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
